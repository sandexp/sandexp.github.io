#### 介绍

1. 一般来说，文件存储系统中包含有成百上千台由便宜组件组成的机器。 假设这些机器是容易损坏的。所以在非永久损坏的情况下，我们需要保证其能从失败中恢复过来，且保留上次宕机留下的状态机信息。因此对于系统的监控，错误发现，容错以及原子恢复就显得非常重要了。

2. 正常情况下管理文件系统管理大文件很正常，当数据达到TB级别之后，就不太合适精细管理KB级别大小的文件了。因此将文件管理的基本单位设计为文件块(Block).

3. 大多数文件都是支持添加的，且不允许覆盖历史数据，而且不允许随机写入. 主要是利用了顺序写，减小磁盘寻道时间的原理.

#### 设计概要

1. 系统由多个便宜的组件组成，且经常宕机，所以需要进行监控，容错以及恢复的操作

2. 系统中存储有大量的文件，大概million的级别，每个文件块大小100 MB左右.因此系统可以管理GB级别的数据，小文件存储需要支持，但是不需要对其进行优化。

3. 关于数据读取:

   >+ 系统允许大量的顺序读取以及少量的随机读取
   >
   >+ 顺序读取的时候，会获取一个文件region的连续区域部分
   >
   >+ 而随机读取可以获取指定位置的部分数据，但是对于性能敏感的应用，需要确定读取的磁盘位置，并对其进行排序后，再进行一次性读取，减少磁盘的来回调度

4. 关于数据的写入：

   > + 系统主要允许连续数据的追加模式，一旦写入，数据很少被修改
   >
   > + 随机写入虽然支持，但是性能很差

5. 关于并发读写：

   > 系统必须要支持向一个文件添加记录的并发处理功能，需要尽可能降低同步操作的开销。

6. 网络的传输带宽相对于低延迟来说更加重要, 因为目标应用经常会把数据按照批次的模式发送


#### 接口功能

1. GFS支持类似文件系统的接口，通过分层的结构实现指定文件的访问(类似于Linux的文件系统结果)

2. 除了基本的文件操作之外, GFS还提供快照和记录添加的功能

   + 快照功能支持在原本的目录下创建一个文件副本

   + 记录添加是指在多线程并发的情境下将记录添加到文件末尾。

#### GFS架构

1. GFS集群包含一个master节点，多个文件块存服务节点chunkserver, 且这些节点可以被多个客户端访问.

2. 文件会被划分成定长的数据块（chunk）, 每个数据块在创建的时候由master去指定一个全局唯一的64位`chunkhandler`

3. 数据块的映射表，数据块的位置信息等等. 同时也控制系统内的活动，例如，数据块管理，垃圾回收，数据块的迁移

   master也会通过心跳机制周期性的去检查chunkserver的状态信息.

4. 客户端会实现文件系统级别的API. 客户端通过与master和chunkserver的交互读取对应的数据. 其中，与master交互元数据的信息，而真正数据读取或者写入工作交给chunkserver.

5. 客户端和chunkserver都不需要缓存文件数据. chunkserver由于Linux存在有缓冲池的设计，所以不需要设计缓存


#### 单主节点架构

1. 我们必须要最小化master节点的读写，这样会造成性能的瓶颈. 客户端从来不会从master节点进行数据的读写

   master只会提供给chunkserver的元数据信息，最终客户端会与chunkserver进行数据的传输工作.

2. 使用定长数据块的数据获取流程：
   元数据交互过程:

   1. 客户端将文件名和字节偏移量转换为文件内部的索引号
   2. 发送给master一条请求，请求信息中包含文件名称和chunk的索引号
   3. master响应给客户端一条消息，包括chunk handle和副本的位置
   4. 客户端将文件名称和chunk的索引号作为key值缓存起来

3. 客户端读取数据流程
   数据传输：

   1. 客户端发送请求给其中一个副本，大概率是最近的一个副本。请求中指定了chunk handle以及数据块中的数据范围。
   2. 在数据传输完毕，或者连接超时的情况下，会通知master对相关元数据进行变更

#### 文件块大小的选择

chunk数据块设计的较大的优势:
1. 大文件块有利于降低网络开销，每次建立TCP连接就会导致一定的网络开销，大文件相对来说建立TCP的次数少，开销较低
2. 降低了客户端与master的通信频率
3. 降低了master中元数据存储数据量的大小，使得master可以将元数据存储在内存中。

大数据块存储的劣势:

会造成读写热点问题，访问的数据总是会命中同一个chunkserver

#### 元数据管理

元数据包含三种类型
1.文件和数据块的命名空间
2.文件与数据块之间的映射关系
3.数据块的存储位置

这些数据都存储在master的内存中. 前两项数据可以通过日志持久化在master本地，用于日志恢复的工作. master不会存储副本信息，但是会要求chunk所属server在加入集群时进行处理.

##### 内存数据结构

对于master来说，后台周期性地扫描整个chunk表的状态很简单。周期性扫描主要用于进行垃圾回收，chunkserver失效时候的副本分配，以及chunk迁移时候的负载均衡。

仅仅依靠内存进行元数据的存储，会由于master节点的内存大小，导致整个集群的规模受限，所以需要增加master的内存才能解决这个问题。

##### 文件块位置信息

master通过对chunkserver的轮询获取chunk位置的信息，所以不需要持久化到日志中。同时通过心跳机制保证master获取最新的位置信息。

##### 操作日志

操作日志中保持了元数据正确修改的历史记录。他除了会保持历史的元数据记录之外，还会作为逻辑时钟，用于定义并发操作的执行顺序。文件，数据块，以及其版本信息，都会被逻辑时钟唯一标记。

由于操作日志是正确的，我们必须将其进行可靠存储，之后才能将其暴露给客户端。因此，我们需要将日志副本到各个远端机器上，成功执行之后，将执行结果返回给客户端。同时使用批量写入的方式进行刷盘，降低刷写开销。

master通过操作日志恢复状态机，如果操作日志过长，则需要使用checkpoint技术，将操作日志存储在本地磁盘上，每次从检查点位置进行恢复。恢复的时候只需要最新的完整的检查点以及后续的日志文件信息即可。



#### 一致性模型

##### GFS 的保证

**Region一致性的定义**

1. 数据修改后文件region的状态和修改类型，修改成功与否以及是否存在与并发修改有关。如果所有客户端看到的数据都是一样的，那么这个文件region就是一致的（无论是从哪个副本读取的）。

2. 只有当文件region是一致的且客户端能够看到它的修改写入成功，才算这个文件region是确定的。

3. 并发成功的修改使得region处于一致性但是没有确定的状态，即数据修改没有被写入。典型的，其包含多个混合的修改片段。

**数据写入的方式**

1. 数据修改可以使用写入和记录添加的方式来实现。

2. 写入就是写入到应用指定的文件偏移量出。

3. 而添加模式与普通的日志添加稍有不同，普通的日志添加时添加到文件末尾，而这里添加的位置是由GFS指定的。

#### 副本同步过程

1. 在一系列成功的数据修改之后，文件region为确定的，且包含有最近修改的数据。

2. GFS通过在副本的chunk上进行相同的动作，来同步副本数据。

3. 通过使用chunk版本信息号来发现失效副本(由于server宕机而导致确实修改信息的副本)

4. 这个副本不能响应master关于chunk元数据的查询，且不能再进行数据修改，应当尽早回收

**客户端缓存对于副本数据获取的影响**

1. 由于客户端缓存了chunk的位置信息，所以在客户端信息更新之前可能会获取到旧的副本数据。

2. 这个时间大小与缓存entry的超时时间与下次文件打开有关。

3. 发生上述两种操作的数据，会清空文件中的所有缓存信息。

**副本恢复过程**

1. 数据成功修改之后，组件的失败会导致数据不一致或者数据消失。

2. GFS通过master与chunkserver之间的握手情况，判断出失效的chunkserver。

3. 一旦出现问题，会尝试从副本中恢复。

4. 当所有副本都失效则客户端应当获取到一条错误消息，而不是有server宕机的信息。



##### 应用的实现

GFS应用可以保证宽松的一致性(最终一致性)，通过下述几项技术来保证:
1. 数据写入的追加模式
2. 检查点机制
3. 写出具有自我校验，自我认证的记录

> + 在一般应用中采用的是追加写的模式，而不是随机写。追加完毕之后修改文件的临时名称为最终文件名称。
>
> + 周期性的进行检查点操作，记录当前应用进行的进度。且检查点操作允许在之前未完成的任务上继续执行。
>
> + 对于一些需要用于生产者-消费者队列的记录数据，我们需要保证至少添加一次的语义，所以需要在记录中额外设计校验码信息，用于去除空数据或者重复数据。



#### 系统交互过程

##### 租约与修改顺序

1. 修改操作是对chunk内容或者元数据的修改，每个修改操作都是应用在chunk副本数。

2. 我们使用租约来维护副本之间的一致性修改。

+ 首先，master对chunk授权一个租约副本，这个副本叫做主副本(primary)。

+ 然后，在主副本上应用修改。

+ 最后，从副本通过修改操作。

租约机制是为了减小master的管理开销设计的，初始超时时间为60s.

+ 只要是chunk被修改，`primay`就能一直受到来自于master的扩展信息，这个扩展信息是跟随在心跳信息之后的。

+ master可以在租约过期之前撤销租约。尽管master丢失了和primary的联系，master可以在旧的租约失效之后，将租约授权给新的副本。

写入过程包含如下的步骤:

1. 客户端请求master

   其中master是持有当前租约信息 以及副本的信息，如果没有chunkserver持有租约信息，master会自己选择一个副本授权

2. master将primary副本的信息回复给客户端，同时也会回复其他副本的位置信息。

   客户端会对这些数据进行缓存，用于未来的修改。除非是primary副本不可获得，或者是这个副本不再持有租约

3. 客户端将数据应用到所有副本上,每个chunkserver会将数据存储到内部的LRU缓存上，直到数据被使用或者被逐出内存. 通过分离数据流和控制流，我们可以基于网络拓扑提升调度数据流的性能。(物理chunkserver是不是主副本)

4. 一旦所有副本获取接收到的数据，客户端会将写请求写入主副本，这个请求可以表述当前副本之前写入的数据。

   主副本通过分配连续的序列号给其接收到的修改。按照顺序应用对自己状态机的修改。

5. 主副本发送写请求到从副本，从副本按照相同的顺序进行数据修改。

6. 所有从副本都对主副本回应，则表示它完成了操作。

7. 副本遇到的错误会汇报给客户端，由于是部分写操作失败，所以主副本以及部分从副本写入成功，这就会导致数据的不一致。这里我们对写入失败的副本进行写请求的重试，直到写入成功，恢复一致状态为止。


GFS客户端代码将一个写操作分割为多个写操作，但是由于多个客户端的写入，会造成写入的交错，这就导致一个共享的数据块内容来自几个客户端的写入，虽然不会造成数据的不一致，但是会使得chunk状态变成未定义状态(undefined)。



##### 数据流(数据流控制流分离)

我们需要对数据流和控制流进行解耦，从而高效的使用网络资源。控制流的流向为 客户端-> 主副本 -> 从副本.数据按照管道的方式送到chunkserver中。我们的目的是完全利用机器的网络带宽，避免高延迟的网络瓶颈，减小数据传输的延迟。

数据是按照链式的方式存储在chunkserver上的，而非是其他的拓扑结构。所以，每个数据的带宽都用于传输数据，而非是分发给其他用户去操作。为了避免网络传输的瓶颈，每台机器发送数据到最近的网络拓扑中最近的机器。(这对应于HDFS中的机架感知策略)。



##### 原子日志添加

1. GFS原子性添加操作叫做记录添加，在传统写入过程1中，客户端指定写入的偏移量位置。

2. 对同一个region的并发写入是不可串行的，这个region中写入的数据包含了不同客户端的分段。

3. 在记录添加中，GFS保证最少一次的语义原子性地添加到文件后面，这个偏移量由GFS选择，并且返回偏移量给客户端.

4. 记录添加对于分布式应用来说是一个重度使用的功能，尤其是多个客户端并发的添加记录到文件中。

5. 客户端需要使用类似分布式锁管理器进行同步操作，这样文件通常是多个生产者和单个消费者队列。包含多个客户端生成的合并结果。
6. 然后，将请求发送给主副本，主副本检查最后一个chunk是否可以容纳这条记录，如果可以，则直接添加到末尾，并让从副本同步数据。否则需要新建一个chunk进行添加，然后从副本进行同步。

7. 如果添加记录失败，客户端会尝试操作。因此，同一块chunk副本可能包含重复的记录，因为GFS只保证 至少一次的 语义。在数据块中同样的数据在不同的副本中，写入的位置偏移量需要一致。新添加的记录偏移量较大，除非是放置到了另一个chunk中了。



#### 快照

**定义**

快照操作是对文件/目录树进行了一个副本操作。用户通过对大量数据集创建一个分支副本  或者  在修改之前对当前状态进行checkpoint。

与AFS类似，我们使用标准的cow策略去实现快照，当master接收到快照请求的时候。首先会取消外部租约(主副本)去进行快照操作.

这个保证子写操作需要和master交互，从而查找租约的持有者（主副本）。这个(写操作的查找租约时间)就会给与master创建chunk副本的好机会。

在租约取消或者过期之后，master会将操作记录到磁盘，然后会将日志应用到内存状态中(通过复制源文件的元数据信息)。然后将新创建的快照文件(也就是元数据的目录树文件)执行同样一块chunk即可。

> + 当客户端首次提出对chunk C的快照进行写入操作时， 会向master发送消息，用于查找当前的租约持有者。
>
> + master会注意到C的引用数量有多个，所以会延迟向客户端的响应，这段时间选择一个新的chunk C'。
>
> + 这个C'是创建出来的，且C'与C在同一台机器上，也就是说，一台机器有一个C副本，就会有一个C’副本
>
> + 这样数据是不需要通过网络传输的。
>
> + 随后master会选择一台机器上的C'副本进行租约授权，且响应给客户端。(拿副本作为租约)



#### 主节点的相关操作

master负责执行所有的命名空间操作。除此之外，还需要管理系统中的chunk副本信息，需要决定新创建的chunk的存储位置问题。协调系统中其他动作，保证chunk可以完全复制，且保证能够负载均衡，且重新什么未使用的空间。

##### 命名空间管理以及锁管理

大量对master操作可能会产生性能问题.

> 例如，副本操作需要master取消被快照覆盖的chunkserver的租约信息. 因此，我们运行多个操作同时进行，使用锁去锁定命名空间区域，保证串行化。

传统文件系统中，会持有每个目录下的数据结构，数据结构中会列举目录中的文件。GFS逻辑上代表着一个命名空间，这个命名空间是一个全路径名称到元数据的查询映射表。通过使用前缀压缩的方式，可以在内存中高效的存储。目录树中的每个节点都有一个相关的读写锁。master操作在执行前会获取相关的锁，可以是读锁，写锁或者是读写锁。

在一个目录下，允许并发修改
例如:

>1. 在同一个目录下并发执行多个文件的创建。
>
>2. 每个创建操作会获取目录的读锁和文件的写锁。
>   获取目录名的读锁是为了防止目录被移除，重命名或者快照。

由于命名空间有多个节点，读写锁需要惰性分配，且如果使用完毕，就需要释放。

锁按照顺序的方式进行分配，用于防止死锁的发生，锁按照命名树的层级进行划分，同一层按照字典顺序划分。



##### 副本替换机制

**容错机制对于副本的要求**

1. 无论是上述哪个方面，都不至于在机器之间传输副本，仅仅是防止磁盘或者机器宕机，且完全使用每台机器的带宽。

2. 同时必须要支持副本的跨机架传播，防止单个机架宕机/掉电造成的后果。

3. 同时这就意味着, 数据块的读请求会占用机架的带宽。

4. 另一方面，写请求必须流向多个机架。

**副本创建，副本重平衡**

当master创建chunk的时候，会选择放置的位置，且初始化空副本。会考虑到下述三个因素:
1. 希望将新副本置于磁盘剩余空间最多的服务器上。
2. 我们希望在每台chunkserver上限值最近创建的副本数量。
尽管创建本身开销较低，但是会造成数据的写繁忙的问题。
3. 需要保证副本跨机架传播

**副本数量过少的处理方案**

只要可用副本数量小于一个给定的值，master就会进行重新副本操作(对一个chunk)

每个chunk会基于如下规则进行重新副本分配
1. 距离目标副本数量有多少差距
2. 将存活文件的chunk重新副本动作提前于最近删除的文件

最后，尽量减小失败对运行应用的影响，这样会阻塞用户进程。

**克隆副本过程**

master会获取最高优先级的chunk且 通过从存在且有效的副本中拷贝数据到这个chunkserver的方式进行克隆。

新副本放置规则类似于创建操作:
1. 均衡磁盘使用
2. 限制单台chunkserver上活动的克隆操作
3. 通过机架传播副本信息


为了保证克隆操作不会造成客户端的压力，master会限制克隆操作的数量。

此外，chunkserver会限制每个克隆操作的带宽，通过调节发送给源chunkserver上的读请求数目

**重平衡过程**

最终，master会周期性的重新平衡副本。

它会检查当前副本的分布，且将副本移动到更好的磁盘空间进行负载均衡。

master会慢慢地向新的chunkserver中填充数据，而不是一次性地向chunkserver中写入数据，这样会导致写繁忙情况的出现。删除数据的时候，master需要选择需要移除的副本，总体来说，倾向于移除负载较重的副本。

##### 垃圾回收过程

文件删除之后，GFS不会立即更新可用的物理存储空间。会在垃圾收集的时候进行惰性清除，这个垃圾清理主要是在文件和chunk级别。

**垃圾回收原理**

1. 当文件被应用删除的时候，master会在日志中记录删除操作。

2. 但是不会立即对文件进行删除，文件仅仅会被标记删除(打上删除状态以及删除时间戳信息)。

3. 在master正常对文件系统命名空间扫描的时候， master会移除超过指定时间的文件信息。

4. 到这儿为止，这个删除文件是可以通过指定文件读取，或者重命名为正常名称的方式，获取文件数据的。

5. 文件从命名空间删除的时候，内存元数据会被清理掉，同时和他相关的chunk也会被移除(物理删除)

在chunk 命名空间的正常扫描中，master会标记孤立的chunk块，且删除这些chunk的元数据。


在每次发送给master的心跳包中，chunkserver会汇报其包含的chunk列表， master会回复需要删除的chunk列表。 由chunkserver去删除chunk的副本信息。

**垃圾回收的一些问题**

存储回收的垃圾回收提供了优雅的删除方式.

1. 在大规模分布式系统中实现简单可靠
  由于chunk创建不一定会在所有chunkserver上成功，会出现master不识别的副本信息(创建失败的)。

  副本删除消息可能会丢失，master就需要重试进行发送。

  垃圾回收机制提高了统一可靠的方式去清理不知道的副本。

2. 会将存储回收作为master的后台进程处理
  例如，命名空间的扫描和与chunkserver的握手

> 注意:
>
> 1. 反复创建或者删除的应用可能不能立即重新使用存储权限。
>
> 2. 如果文件又被删除，我们通过加速存储回收的方式处理这个问题。
>
> 3. 我们也可以允许用户在不同的命名空间区域中，使用不同的副本和回收策略。

##### 过期副本发现

1. chunkserver宕机的时候, chunkserver失效且丢失了对chunk的修改的情况下，chunk副本会过期。

2. 对于每个chunk, master维护了chunk版本号信息，用于分辨副本是否是最新副本。

3. 无论何时master授权租约给一个chunk，它都会增加chunk的版本号，且提醒最新副本。

4. master和这些副本都会将新版本号信息记录在持久化状态中。

5. 这个过程发生在客户端收到通知之前，因此也在启动写操作之前。

6. 如果某个副本不可以，那么他的版本号就不会继续向上增加，master会得知他是一个过期副本。(发生在chunkserver重启且汇报chunk列表的时候)

7. 如果master在列表中发现有版本号大于这个的chunk副本存在，master就会认为这个副本过期了，会采用高版本的副本作为最新副本。

8. master会在垃圾回收的过程中移除过期副本。

9. 在移除之前，应对客户端请求的时候，master会将过期副本视作不存在。

10. 当执行租约授权，或者是chunkserver从其他chunkserver上克隆数据的时候，master将chunk版本号包含在请求信息中。



#### 容错与监控

在分布式系统中，组件宕机时很正常的事情，我们既不能完全相信机器，也不能完全相信磁盘。

组件失败会导致系统的不可用，所以需要进行容错处理，同时需要考虑到这些故障的监测。

##### 高可用(HA)

我们通过快速回复和副本机制保证全局的高可用。

##### 快速恢复

master和chunkserver都需要存储状态信息，且无论怎么终止都需要在短时间内恢复。

实际上我们不能清晰的判断出正常终止和非正常终止。如果服务器kill终止，那么客户端请求会失败，并且反复重新连接。

##### 数据块副本

每个chunk会在不同机架的多个chunkserver上进行副本设置。用户可以为不同的文件命名空间 指定副本级别。

master克隆以及存在的副本，因为master需要保证chunk完全的复制(当chunkserver下线或者是过期副本的时候，主要是通过校验和来实现)

尽管副本机制效果很好，但是还是需要其他的跨服务器冗余方案。

> 例如，对于新增的只读代码，需要设置等价或者代码擦除的功能。
>
> 由于性能瓶颈是顺序添加和读取操作，所以在松散耦合系统中 实现冗余是可行的。

##### 主节点副本

1. master状态的复制时用于可靠性的保证，它的操作日志和检查点会被复制到多台机器上。

2. 当本地副本和远程副本都提交成功的时候，状态才会被视作提交成功。

3. 简单来说，一个master进程保留对修改和后台进程的控制，例如垃圾回收的工作。

4. 如果机器或者磁盘失效了，GFS外部的监视设置会启动一个新的master进程(通过操作日志).

> 除此之外，影子master提供文件系统的只读权限(当主master宕机也可以提供服务)。它是影子master，不是镜像，所以可能会稍微落后于主master。它会提高文件的读可用性。
>
> 由于文件内容从chunkserver中获取，应用不会观察到文件内容的过期。但是可以观察到文件元数据短暂的过期，就像目录内容或者权限控制信息。
>

一个影子master通过读取增长的操作日志，并且应用与主master相同的操作序列到日志中。启动的时候，会轮询chunkserver，用于定位chunk位置，同时交换握手信息，用于监控状态。


主节点仅仅用于更新副本位置，这个操作来自于主副本的创建或者删除。

#### 数据整合

1. 每个chunkserver使用校验码，用于发现存储的过期数据。

2. 当我们通过其他chunkserver恢复过期数据的时候，就没办法通过比较副本来发现过期数据了(因为都一样了).

3. chunkserver必须度量的验证副本的完整性(通过校验码)

> chunk大小为64 KB的数据块，每个chunk包含32位的校验码，校验码会存储在内存中，通过日志的形式持久化。

4. chunkserver会验证数据块的校验和，然后才会响应请求。

5. 因此，chunkserver不会传播过期数据给其他机器，因为chunkserver会返回错误给请求者并且汇报不匹配的消息给master。

6. master收到之后，会会从其他机器上读取副本，并且指示过期的chunkserver去删除过期副本。

> 校验码对于读操作的性能有一些影响。因为大多数读取至少扫描一些数据块，我们需要溢写校验码存储空间用于验证，所以这部分数据也需要读取。
>
> GFS客户端代码会降低开销(通过对其读取)
> 此外，校验码差按照和比较不需要IO，校验码计算会被IO覆盖.

7. 对于添加写入的操作的校验码计算是经过优化的。我们进行更新上次计算的部分数据块的校验码，且计算新添加数据的校验码。

8. 尽管上次计算的校验码已经是过期的，且我们并不能立马发现，新的校验码值不匹配数据，过期会在下次读取的时候被发现。

9. 如果一个写操作是范围相关的，我们必须要对首个和末尾chunk进行验证，然后进行数据写入，最后，计算并且记录新的校验码。如果我们不对首位数据块进行验证，新的校验码数据中会包含 没有重写的内容。

10. 空载的时候，chunkserver会扫描并且验证非活跃状态的内容。这个允许我们发现数据块中很少读取的过期数据。一旦发现，master会创建一个新的副本，同时删除过期副本。

##### 监视工具

监视日志可以方便的帮助定位，性能分析的工作，仅仅消耗一些小小的代价而已。GFS会生成监视日志，监视日志会记录重要的时间 和所有RPC请求和响应. 监视日志可以在不影响系统的工作情况下删除。

RPC日志包含了额外的请求和响应信息。通过比较不同机器上的请求和响应记录，可以重构完整的历史数据，用于定位问题。日志可以用于追踪，用于加载测试和性能分析。

由于日志是串行异步写入的，所以性能影响不大。